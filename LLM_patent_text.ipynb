{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlorSbccWEDa"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from google.colab import drive\n",
        "import os\n",
        "from google.colab import files\n",
        "from docx import Document\n",
        "import re"
      ],
      "metadata": {
        "id": "nq0vKGFeW1KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "yKaKB_GjWKjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuSRy_lbWfE3",
        "outputId": "cfe86c75-e730-444f-8e36-61b2ff09df14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reader = PdfReader('/content/gdrive/My Drive/Copy of energy.pdf')"
      ],
      "metadata": {
        "id": "NalD3XkQWrJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_text = ''\n",
        "# for i, page in enumerate(reader.pages):\n",
        "#     text = page.extract_text()\n",
        "#     if text:\n",
        "#         raw_text += text\n",
        "\n",
        "# text_splitter = CharacterTextSplitter(\n",
        "#     separator = \"\\n\",\n",
        "#     chunk_size = 1000,\n",
        "#     chunk_overlap  = 200,\n",
        "#     length_function = len,\n",
        "# )\n",
        "# texts = text_splitter.split_text(raw_text)\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "# docsearch = FAISS.from_texts(texts, embeddings)\n",
        "# chain = load_qa_chain(ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "2VXlucKiW7bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"what is the paper's topic?\"\n",
        "# docs = docsearch.similarity_search(query)\n",
        "# chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3mtAth2jXNKO",
        "outputId": "604eabb1-a261-4342-c02f-7dd36d346258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The paper's topic is the controversy and reorganization plans surrounding atomic power and nuclear weapons development and testing.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# auth.authenticate_user()\n",
        "# creds, _ = default()\n",
        "# gc = gspread.authorize(creds)\n",
        "# worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1fYvhaE5R2H91H3Pj8uoE4rzECMm6v1Y9TQm4QJlw_Hg/edit#gid=0').sheet1"
      ],
      "metadata": {
        "id": "xSvUXzKMJUcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def run_query_and_append(query):\n",
        "#     docs = docsearch.similarity_search(query)\n",
        "#     answer = chain.run(input_documents=docs, question=query)\n",
        "#     worksheet.append_row([query, str(answer)])\n",
        "#     rows = worksheet.get_all_values()\n",
        "#     df = pd.DataFrame.from_records(rows, columns=rows[0])\n",
        "#     df = df.drop(0)\n",
        "#     return df"
      ],
      "metadata": {
        "id": "yeDDwuPzd7EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def run_query_and_append(query):\n",
        "#     # Assuming docs and chain.run are defined elsewhere and properly initialized\n",
        "#     docs = docsearch.similarity_search(query)\n",
        "#     answer = chain.run(input_documents=docs, question=query)\n",
        "\n",
        "#     # Split the returned scores and format them into a list\n",
        "#     scores = answer.split(\",\")  # Adjust splitting based on actual delimiter/format used in 'answer'\n",
        "#     row_data = [query] + scores  # Prepend the query to the scores\n",
        "\n",
        "#     # Append row to the worksheet\n",
        "#     worksheet.append_row(row_data)\n",
        "\n",
        "#     # Fetch updated data from the worksheet\n",
        "#     rows = worksheet.get_all_values()\n",
        "#     df = pd.DataFrame.from_records(rows, columns=rows[0])\n",
        "#     df = df.drop(0)  # Drop the header row now used as column names\n",
        "#     return df"
      ],
      "metadata": {
        "id": "Ccvz_RbouquU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"if you want to score that text related to each of these category, what do you tell? these are categories: 1. Agriculture and Food, 2. Chemistry and Metallurgy, 3. Consumer Goods, 4. Electricity and Electronics, 5. Engineering and Construction and Mining, 6. Health and Entertainment, 7. Instruments and Information, 8. Lighting and Heating and Nuclear, 9. Manufacturing Process, 10. Transportation, 11. Weapons (sum of all score should be 1,only return 11 numbers)\"\n",
        "# updated_df = run_query_and_append(query)"
      ],
      "metadata": {
        "id": "571dHkA8WnWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"if you want to score that text related to each of these category, what do you tell? these are categories: 1. Agriculture and Food, 2. Chemistry and Metallurgy, 3. Consumer Goods, 4. Electricity and Electronics, 5. Engineering and Construction and Mining, 6. Health and Entertainment, 7. Instruments and Information, 8. Lighting and Heating and Nuclear, 9. Manufacturing Process, 10. Transportation, 11. Weapons (sum of all score should be 1,only return 11 numbers)\"\n",
        "# docs = docsearch.similarity_search(query)\n",
        "# chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IYggl3wBvlAX",
        "outputId": "eb23a185-a97d-413f-e3a5-03c7425af51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Agriculture and Food: 0\\n2. Chemistry and Metallurgy: 0.05\\n3. Consumer Goods: 0\\n4. Electricity and Electronics: 0.05\\n5. Engineering and Construction and Mining: 0.15\\n6. Health and Entertainment: 0\\n7. Instruments and Information: 0.05\\n8. Lighting and Heating and Nuclear: 0.55\\n9. Manufacturing Process: 0.10\\n10. Transportation: 0\\n11. Weapons: 0.15'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# def run_query_and_get_df(query):\n",
        "#     # Simulate fetching documents relevant to the query\n",
        "#     docs = docsearch.similarity_search(query)\n",
        "\n",
        "#     # Assuming chain.run returns a formatted string answer\n",
        "#     answer = chain.run(input_documents=docs, question=query)\n",
        "\n",
        "#     # Split the answer into lines and extract scores\n",
        "#     lines = answer.split('\\n')\n",
        "#     scores = [float(line.split(': ')[1]) for line in lines if ': ' in line]\n",
        "\n",
        "#     # Define column names based on the categories\n",
        "#     columns = [\n",
        "#         'Agriculture and Food', 'Chemistry and Metallurgy', 'Consumer Goods',\n",
        "#         'Electricity and Electronics', 'Engineering and Construction and Mining',\n",
        "#         'Health and Entertainment', 'Instruments and Information',\n",
        "#         'Lighting and Heating and Nuclear', 'Manufacturing Process',\n",
        "#         'Transportation', 'Weapons'\n",
        "#     ]\n",
        "\n",
        "#     # Create a DataFrame\n",
        "#     df = pd.DataFrame([scores], columns=columns)\n",
        "#     return df\n",
        "\n",
        "# # Example usage\n",
        "# query = \"if you want to score that text related to each of these category, what do you tell? these are categories: 1. Agriculture and Food, 2. Chemistry and Metallurgy, 3. Consumer Goods, 4. Electricity and Electronics, 5. Engineering and Construction and Mining, 6. Health and Entertainment, 7. Instruments and Information, 8. Lighting and Heating and Nuclear, 9. Manufacturing Process, 10. Transportation, 11. Weapons (sum of all score should be 1, only return 11 numbers)\"\n",
        "# df = run_query_and_get_df(query)\n",
        "# print(df)\n"
      ],
      "metadata": {
        "id": "Do82Wo2TwlHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader  # Assuming you are using PyPDF2 or similar\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    raw_text = ''\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            raw_text += text\n",
        "    return raw_text\n",
        "\n",
        "def run_query_and_get_df(texts, query):\n",
        "    # Your embeddings, docsearch, and chain setup should be called here for each file\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    docsearch = FAISS.from_texts(texts, embeddings)\n",
        "    chain = load_qa_chain(ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\")\n",
        "    docs = docsearch.similarity_search(query)\n",
        "    answer = chain.run(input_documents=docs, question=query)\n",
        "\n",
        "    lines = answer.split('\\n')\n",
        "    scores = [float(line.split(': ')[1]) for line in lines if ': ' in line]\n",
        "    columns = [\n",
        "        'Agriculture and Food', 'Chemistry and Metallurgy', 'Consumer Goods',\n",
        "        'Electricity and Electronics', 'Engineering and Construction and Mining',\n",
        "        'Health and Entertainment', 'Instruments and Information',\n",
        "        'Lighting and Heating and Nuclear', 'Manufacturing Process',\n",
        "        'Transportation', 'Weapons'\n",
        "    ]\n",
        "    df = pd.DataFrame([scores], columns=columns)\n",
        "    return df\n",
        "\n",
        "folder_path = '/content/gdrive/My Drive/NASA_pdf'\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "df_nasa = pd.DataFrame()\n",
        "\n",
        "# Define your query outside the loop so it doesn't have to be redefined for each iteration\n",
        "#query = \"if you want to score that text related to each of these category, what do you tell? these are categories: 1. Agriculture and Food, 2. Chemistry and Metallurgy, 3. Consumer Goods, 4. Electricity and Electronics, 5. Engineering and Construction and Mining, 6. Health and Entertainment, 7. Instruments and Information, 8. Lighting and Heating and Nuclear, 9. Manufacturing Process, 10. Transportation, 11. Weapons (sum of all score should be 1, only return 11 numbers, please return no explain just scores. and you have to score them. try your best. the format of the output should be category then ':' then score then go to next line.)\"\n",
        "query = (\n",
        "    \"I need scores for each of these categories, represented as a decimal proportion of a total sum of 1. \"\n",
        "    \"Please provide the scores in a simple format without any additional explanations. List each category \"\n",
        "    \"followed by a colon and then the score, with each category on a new line. The categories are: \"\n",
        "    \"1. Agriculture and Food, 2. Chemistry and Metallurgy, 3. Consumer Goods, 4. Electricity and Electronics, \"\n",
        "    \"5. Engineering and Construction and Mining, 6. Health and Entertainment, 7. Instruments and Information, \"\n",
        "    \"8. Lighting and Heating and Nuclear, 9. Manufacturing Process, 10. Transportation, 11. Weapons. \"\n",
        "    \"Format the output exactly as follows: '<Category name>: <score>', with each on a separate line. \"\n",
        "    \"Ensure the scores sum to 1. Thank you!\"\n",
        ")\n",
        "# Process each PDF file\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    raw_text = extract_text_from_pdf(file_path)\n",
        "    # Assuming you have a text splitting method as per your initial setup\n",
        "    # This part is simplified here; implement your actual text splitting logic if needed\n",
        "    texts = [raw_text]  # Directly use raw_text if no need for further splitting\n",
        "    df_file = run_query_and_get_df(texts, query)\n",
        "    df_file['Source'] = file_name  # Optionally add source file name to track data\n",
        "    df_nasa = pd.concat([df_nasa, df_file], ignore_index=True)\n",
        "\n",
        "#print(df_main)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "dAJeWO4Q6gno",
        "outputId": "d53ad84e-3318-4e90-b3e0-789af00da078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "11 columns passed, passed data had 0 columns",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0;31m# caller's responsibility to check for this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             raise AssertionError(\n\u001b[0m\u001b[1;32m    982\u001b[0m                 \u001b[0;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 11 columns passed, passed data had 0 columns",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-2c3e116931d4>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# This part is simplified here; implement your actual text splitting logic if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Directly use raw_text if no need for further splitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_query_and_get_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mdf_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Source'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m  \u001b[0;31m# Optionally add source file name to track data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdf_nasa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_nasa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-2c3e116931d4>\u001b[0m in \u001b[0;36mrun_query_and_get_df\u001b[0;34m(texts, query)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m'Transportation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Weapons'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     ]\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    780\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    783\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_finalize_columns_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;31m# GH#26429 do not raise user-facing AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 11 columns passed, passed data had 0 columns"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader  # Assuming you are using PyPDF2 or similar\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    raw_text = ''\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            raw_text += text\n",
        "    return raw_text\n",
        "\n",
        "def run_query_and_get_df(texts, query):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    docsearch = FAISS.from_texts(texts, embeddings)\n",
        "    chain = load_qa_chain(ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\")\n",
        "    docs = docsearch.similarity_search(query)\n",
        "    answer = chain.run(input_documents=docs, question=query)\n",
        "\n",
        "    lines = answer.split('\\n')\n",
        "    print(\"Debug - Lines:\", lines)  # Debug output\n",
        "    scores = []\n",
        "\n",
        "    for line in lines:\n",
        "        if ': ' in line:\n",
        "            parts = line.split(': ')\n",
        "            if len(parts) > 1:\n",
        "                try:\n",
        "                    score = float(parts[1])\n",
        "                    scores.append(score)\n",
        "                except ValueError:\n",
        "                    print(\"Debug - Failed to convert:\", parts[1])  # Debug output\n",
        "\n",
        "    print(\"Debug - Scores:\", scores)  # Debug output\n",
        "\n",
        "    columns = [\n",
        "        'Agriculture and Food', 'Chemistry and Metallurgy', 'Consumer Goods',\n",
        "        'Electricity and Electronics', 'Engineering and Construction and Mining',\n",
        "        'Health and Entertainment', 'Instruments and Information',\n",
        "        'Lighting and Heating and Nuclear', 'Manufacturing Process',\n",
        "        'Transportation', 'Weapons'\n",
        "    ]\n",
        "\n",
        "    if len(scores) == 11:\n",
        "        df = pd.DataFrame([scores], columns=columns)\n",
        "    else:\n",
        "        print(\"Error: Expected 11 scores, received\", len(scores))  # Error message\n",
        "        df = pd.DataFrame(columns=columns)  # Return empty DataFrame with correct columns\n",
        "\n",
        "    return df\n",
        "\n",
        "folder_path = '/content/gdrive/My Drive/agency'\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "# df_nasa = pd.DataFrame()\n",
        "\n",
        "# Define your query outside the loop so it doesn't have to be redefined for each iteration\n",
        "query = \"if you want to score that text related to each of these category, what do you tell? these are categories: 1. Agriculture and Food, 2. Chemistry and Metallurgy, 3. Consumer Goods, 4. Electricity and Electronics, 5. Engineering and Construction and Mining, 6. Health and Entertainment, 7. Instruments and Information, 8. Lighting and Heating and Nuclear, 9. Manufacturing Process, 10. Transportation, 11. Weapons (sum of all score should be 1, only return 11 numbers, please return no explain just scores. and you have to score them. try your best. the format of the output should be category then ':' then score then go to next line.) please force to score\"\n",
        "\n",
        "# # Process each PDF file\n",
        "# for file_name in files:\n",
        "#     file_path = os.path.join(folder_path, file_name)\n",
        "#     raw_text = extract_text_from_pdf(file_path)\n",
        "#     # Assuming you have a text splitting method as per your initial setup\n",
        "#     # This part is simplified here; implement your actual text splitting logic if needed\n",
        "#     texts = [raw_text]  # Directly use raw_text if no need for further splitting\n",
        "#     df_file = run_query_and_get_df(texts, query)\n",
        "#     df_file['Source'] = file_name  # Optionally add source file name to track data\n",
        "#     df_nasa = pd.concat([df_nasa, df_file], ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qwdtttGoNnW",
        "outputId": "c4c5645c-4d37-4eb0-d189-658e2171d7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(df_main)\n",
        "# Define the path for the output text file\n",
        "output_file_path = '/content/gdrive/My Drive/agency_scores.txt'\n",
        "\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        raw_text = extract_text_from_pdf(file_path)\n",
        "        # Assuming you have a text splitting method as per your initial setup\n",
        "        # This part is simplified here; implement your actual text splitting logic if needed\n",
        "        texts = [raw_text]  # Directly use raw_text if no need for further splitting\n",
        "        df_file = run_query_and_get_df(texts, query)\n",
        "        scores = df_file.iloc[0].values.tolist()  # Convert DataFrame row to list\n",
        "        scores_text = ', '.join(map(str, scores))\n",
        "        output_file.write(f\"{scores_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzRR8wL2285c",
        "outputId": "92e9ab3f-8c0d-431c-a69f-7b73b057ac79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug - Lines: ['1. Agriculture and Food: 0.00', '2. Chemistry and Metallurgy: 0.05', '3. Consumer Goods: 0.05', '4. Electricity and Electronics: 0.10', '5. Engineering and Construction and Mining: 0.10', '6. Health and Entertainment: 0.10', '7. Instruments and Information: 0.20', '8. Lighting and Heating and Nuclear: 0.10', '9. Manufacturing Process: 0.10', '10. Transportation: 0.15', '11. Weapons: 0.05']\n",
            "Debug - Scores: [0.0, 0.05, 0.05, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.15, 0.05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defence"
      ],
      "metadata": {
        "id": "QgmC9A7gwqwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "nkPwqcftBdNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from docx import Document\n",
        "\n",
        "def save_split_documents(file_path, title_pattern, drive_path='/content/gdrive/My Drive/app/'):\n",
        "    doc = Document(file_path)\n",
        "    new_doc = None\n",
        "    file_name = None  # Initialize filename variable\n",
        "    doc_index = 0  # Start index from 0\n",
        "    title_regex = re.compile(title_pattern)\n",
        "    year_pattern = re.compile(r'(?:FY|Fiscal Year)\\s*(\\d{4})')\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if paragraph.style.name.startswith('Heading') and title_regex.search(paragraph.text):\n",
        "            if new_doc:  # Save the current document before starting a new one\n",
        "                new_doc.save(drive_path + file_name)\n",
        "                doc_index += 1\n",
        "\n",
        "            match = year_pattern.search(paragraph.text)  # Search for the year within the heading\n",
        "            if match:\n",
        "                year = match.group(1)  # Extract the year\n",
        "                file_name = f'Defense_{year}.docx'  # Update the file name based on the year\n",
        "            else:\n",
        "                file_name = 'Defense_Unknown.docx'  # Default file name if year not found\n",
        "\n",
        "            new_doc = Document()  # Start a new document\n",
        "            new_doc.add_paragraph(paragraph.text)  # Add the heading to the new document\n",
        "        elif new_doc:  # Continue adding paragraphs to the current document\n",
        "            new_doc.add_paragraph(paragraph.text)\n",
        "\n",
        "    if new_doc:  # Save the last document if it exists\n",
        "        new_doc.save(drive_path + file_name)\n",
        "\n",
        "    return doc_index + 1  # Return the count of documents created\n",
        "\n",
        "# Code to upload and process the document\n",
        "filename = next(iter(uploaded))  # Assumes one file is uploaded\n",
        "title_pattern = r'(R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})|(Department of Defense R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})'\n",
        "doc_count = save_split_documents(filename, title_pattern)\n",
        "print(f\"Number of documents created: {doc_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdpZCQpHrLPu",
        "outputId": "04f23a96-88c7-494c-ac34-50f5d2d8b926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents created: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NASA"
      ],
      "metadata": {
        "id": "_8WTc8aNww77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Dye8J1-kwzoV",
        "outputId": "ba9e9cf0-9992-4788-beb9-74aabbfc2d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-63f9ffca-5d47-4ec5-b428-dcc57fcd9233\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-63f9ffca-5d47-4ec5-b428-dcc57fcd9233\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NASA.docx to NASA.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from docx import Document\n",
        "\n",
        "def save_split_documents(file_path, title_pattern, drive_path='/content/gdrive/My Drive/NASA/'):\n",
        "    doc = Document(file_path)\n",
        "    new_doc = None\n",
        "    file_name = None  # Initialize filename variable\n",
        "    doc_index = 0  # Start index from 0\n",
        "    title_regex = re.compile(title_pattern)\n",
        "    year_pattern = re.compile(r'(?:FY|Fiscal Year)\\s*(\\d{4})')\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if paragraph.style.name.startswith('Heading') and title_regex.search(paragraph.text):\n",
        "            if new_doc:  # Save the current document before starting a new one\n",
        "                new_doc.save(drive_path + file_name)\n",
        "                doc_index += 1\n",
        "\n",
        "            match = year_pattern.search(paragraph.text)  # Search for the year within the heading\n",
        "            if match:\n",
        "                year = match.group(1)  # Extract the year\n",
        "                file_name = f'NASA_{year}.docx'  # Update the file name based on the year\n",
        "            else:\n",
        "                file_name = 'NASA_Unknown.docx'  # Default file name if year not found\n",
        "\n",
        "            new_doc = Document()  # Start a new document\n",
        "            new_doc.add_paragraph(paragraph.text)  # Add the heading to the new document\n",
        "        elif new_doc:  # Continue adding paragraphs to the current document\n",
        "            new_doc.add_paragraph(paragraph.text)\n",
        "\n",
        "    if new_doc:  # Save the last document if it exists\n",
        "        new_doc.save(drive_path + file_name)\n",
        "\n",
        "    return doc_index + 1  # Return the count of documents created\n",
        "\n",
        "# Code to upload and process the document\n",
        "filename = next(iter(uploaded))  # Assumes one file is uploaded\n",
        "title_pattern = r'(R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})|(NASA R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})'\n",
        "doc_count = save_split_documents(filename, title_pattern)\n",
        "print(f\"Number of documents created: {doc_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80d5752-473b-4fa3-d880-21a93e0e5037",
        "id": "0H8P4ygVwzoW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents created: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NIH"
      ],
      "metadata": {
        "id": "308ruYy4zSWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "2aca401b-dd36-4217-99fa-010c0ba59d72",
        "id": "CcTRlKHBzQ8L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a96217d7-7983-491a-9ef8-04fdb50c54ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a96217d7-7983-491a-9ef8-04fdb50c54ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NIH.docx to NIH.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from docx import Document\n",
        "\n",
        "def save_split_documents(file_path, title_pattern, drive_path='/content/gdrive/My Drive/NIH/'):\n",
        "    doc = Document(file_path)\n",
        "    new_doc = None\n",
        "    file_name = None  # Initialize filename variable\n",
        "    doc_index = 0  # Start index from 0\n",
        "    title_regex = re.compile(title_pattern)\n",
        "    year_pattern = re.compile(r'(?:FY|Fiscal Year)\\s*(\\d{4})')\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if paragraph.style.name.startswith('Heading') and title_regex.search(paragraph.text):\n",
        "            if new_doc:  # Save the current document before starting a new one\n",
        "                new_doc.save(drive_path + file_name)\n",
        "                doc_index += 1\n",
        "\n",
        "            match = year_pattern.search(paragraph.text)  # Search for the year within the heading\n",
        "            if match:\n",
        "                year = match.group(1)  # Extract the year\n",
        "                file_name = f'NIH_{year}.docx'  # Update the file name based on the year\n",
        "            else:\n",
        "                file_name = 'NIH_Unknown.docx'  # Default file name if year not found\n",
        "\n",
        "            new_doc = Document()  # Start a new document\n",
        "            new_doc.add_paragraph(paragraph.text)  # Add the heading to the new document\n",
        "        elif new_doc:  # Continue adding paragraphs to the current document\n",
        "            new_doc.add_paragraph(paragraph.text)\n",
        "\n",
        "    if new_doc:  # Save the last document if it exists\n",
        "        new_doc.save(drive_path + file_name)\n",
        "\n",
        "    return doc_index + 1  # Return the count of documents created\n",
        "\n",
        "# Code to upload and process the document\n",
        "filename = next(iter(uploaded))  # Assumes one file is uploaded\n",
        "title_pattern = r'(R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})|(NIH R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})'\n",
        "doc_count = save_split_documents(filename, title_pattern)\n",
        "print(f\"Number of documents created: {doc_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1721cdf-93d7-4555-c64b-87d9bae3d4df",
        "id": "K_USQOeJzQ8M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents created: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSF"
      ],
      "metadata": {
        "id": "b5zc-Rob2Mpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c97448fd-f26d-46b4-e5bd-e96e5efc3478",
        "id": "oGXf3s_52OOg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30a85fbc-12a5-4c7d-b5b1-6fb38615315b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-30a85fbc-12a5-4c7d-b5b1-6fb38615315b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NSF.docx to NSF.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from docx import Document\n",
        "\n",
        "def save_split_documents(file_path, title_pattern, drive_path='/content/gdrive/My Drive/NSF/'):\n",
        "    doc = Document(file_path)\n",
        "    new_doc = None\n",
        "    file_name = None  # Initialize filename variable\n",
        "    doc_index = 0  # Start index from 0\n",
        "    title_regex = re.compile(title_pattern)\n",
        "    year_pattern = re.compile(r'(?:FY|Fiscal Year)\\s*(\\d{4})')\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if paragraph.style.name.startswith('Heading') and title_regex.search(paragraph.text):\n",
        "            if new_doc:  # Save the current document before starting a new one\n",
        "                new_doc.save(drive_path + file_name)\n",
        "                doc_index += 1\n",
        "\n",
        "            match = year_pattern.search(paragraph.text)  # Search for the year within the heading\n",
        "            if match:\n",
        "                year = match.group(1)  # Extract the year\n",
        "                file_name = f'NSF_{year}.docx'  # Update the file name based on the year\n",
        "            else:\n",
        "                file_name = 'NSF_Unknown.docx'  # Default file name if year not found\n",
        "\n",
        "            new_doc = Document()  # Start a new document\n",
        "            new_doc.add_paragraph(paragraph.text)  # Add the heading to the new document\n",
        "        elif new_doc:  # Continue adding paragraphs to the current document\n",
        "            new_doc.add_paragraph(paragraph.text)\n",
        "\n",
        "    if new_doc:  # Save the last document if it exists\n",
        "        new_doc.save(drive_path + file_name)\n",
        "\n",
        "    return doc_index + 1  # Return the count of documents created\n",
        "\n",
        "# Code to upload and process the document\n",
        "filename = next(iter(uploaded))  # Assumes one file is uploaded\n",
        "title_pattern = r'(R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})|(NSF R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})'\n",
        "doc_count = save_split_documents(filename, title_pattern)\n",
        "print(f\"Number of documents created: {doc_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47357f77-ae97-429c-a3f7-5083be638187",
        "id": "b-LRlwvn2OOg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents created: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy"
      ],
      "metadata": {
        "id": "MlZN5anv4s1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "be01f7e1-05c9-4ba5-d1b9-86fc65884264",
        "id": "fb_31stp4u9D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-01b5929e-8a9d-421e-9c6b-0d33b8f5c21a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-01b5929e-8a9d-421e-9c6b-0d33b8f5c21a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Energy.docx to Energy.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from docx import Document\n",
        "\n",
        "def save_split_documents(file_path, title_pattern, drive_path='/content/gdrive/My Drive/Energy/'):\n",
        "    doc = Document(file_path)\n",
        "    new_doc = None\n",
        "    file_name = None  # Initialize filename variable\n",
        "    doc_index = 0  # Start index from 0\n",
        "    title_regex = re.compile(title_pattern)\n",
        "    year_pattern = re.compile(r'(?:FY|Fiscal Year)\\s*(\\d{4})')\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if paragraph.style.name.startswith('Heading') and title_regex.search(paragraph.text):\n",
        "            if new_doc:  # Save the current document before starting a new one\n",
        "                new_doc.save(drive_path + file_name)\n",
        "                doc_index += 1\n",
        "\n",
        "            match = year_pattern.search(paragraph.text)  # Search for the year within the heading\n",
        "            if match:\n",
        "                year = match.group(1)  # Extract the year\n",
        "                file_name = f'Energy_{year}.docx'  # Update the file name based on the year\n",
        "            else:\n",
        "                file_name = 'Energy_Unknown.docx'  # Default file name if year not found\n",
        "\n",
        "            new_doc = Document()  # Start a new document\n",
        "            new_doc.add_paragraph(paragraph.text)  # Add the heading to the new document\n",
        "        elif new_doc:  # Continue adding paragraphs to the current document\n",
        "            new_doc.add_paragraph(paragraph.text)\n",
        "\n",
        "    if new_doc:  # Save the last document if it exists\n",
        "        new_doc.save(drive_path + file_name)\n",
        "\n",
        "    return doc_index + 1  # Return the count of documents created\n",
        "\n",
        "# Code to upload and process the document\n",
        "filename = next(iter(uploaded))  # Assumes one file is uploaded\n",
        "title_pattern = r'(R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})|(Energy R&D Appropriations for (FY|Fiscal Year)\\s*\\d{4})'\n",
        "doc_count = save_split_documents(filename, title_pattern)\n",
        "print(f\"Number of documents created: {doc_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe87086-6da5-4646-a381-5f7e6bc6da23",
        "id": "4bLvoHWs4u9E"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents created: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# to pdf"
      ],
      "metadata": {
        "id": "SHMcbolf5qVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y pandoc\n",
        "!apt-get install -y wkhtmltopdf\n",
        "!pip install pypandoc\n",
        "!pip install pdfkit"
      ],
      "metadata": {
        "id": "AMrzii5dWBfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pypandoc\n",
        "import pdfkit\n",
        "\n",
        "def convert_docx_to_pdf(docx_path, pdf_path):\n",
        "    html = pypandoc.convert_file(docx_path, 'html')\n",
        "    pdfkit.from_string(html, pdf_path)\n",
        "\n",
        "def convert_directory(docx_dir, pdf_dir):\n",
        "    # Create target directory for PDFs if it does not exist\n",
        "    if not os.path.exists(pdf_dir):\n",
        "        os.makedirs(pdf_dir)\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(docx_dir):\n",
        "        if filename.endswith(\".docx\") or filename.endswith(\".DOCX\"):\n",
        "            docx_path = os.path.join(docx_dir, filename)\n",
        "            pdf_filename = filename.replace(\".docx\", \".pdf\").replace(\".DOCX\", \".pdf\")\n",
        "            pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
        "            convert_docx_to_pdf(docx_path, pdf_path)\n",
        "            print(f\"Converted: {pdf_filename}\")\n"
      ],
      "metadata": {
        "id": "EOyqS0rn8jr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive'\n",
        "agencies = ['NASA', 'NIH', 'NSF', 'Energy', 'Defence']\n",
        "\n",
        "for agency in agencies:\n",
        "    docx_dir = os.path.join(base_dir, agency)\n",
        "    pdf_dir = os.path.join(base_dir, f\"{agency}_pdf\")\n",
        "    convert_directory(docx_dir, pdf_dir)\n",
        "    print(f\"Conversion completed for {agency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJkiwZ2JWDhR",
        "outputId": "6eb0b8bd-d43e-437a-c2c9-bd49b76cd3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted: NASA_1958.pdf\n",
            "Converted: NASA_1959.pdf\n",
            "Converted: NASA_1960.pdf\n",
            "Converted: NASA_1961.pdf\n",
            "Converted: NASA_1962.pdf\n",
            "Converted: NASA_1963.pdf\n",
            "Converted: NASA_1964.pdf\n",
            "Converted: NASA_1967.pdf\n",
            "Converted: NASA_1968.pdf\n",
            "Converted: NASA_1969.pdf\n",
            "Converted: NASA_1970.pdf\n",
            "Converted: NASA_1971.pdf\n",
            "Converted: NASA_1972.pdf\n",
            "Converted: NASA_1974.pdf\n",
            "Converted: NASA_1975.pdf\n",
            "Converted: NASA_1979.pdf\n",
            "Converted: NASA_1980.pdf\n",
            "Converted: NASA_1981.pdf\n",
            "Converted: NASA_1983.pdf\n",
            "Converted: NASA_1987.pdf\n",
            "Converted: NASA_1988.pdf\n",
            "Converted: NASA_1989.pdf\n",
            "Converted: NASA_1990.pdf\n",
            "Converted: NASA_1991.pdf\n",
            "Converted: NASA_1995.pdf\n",
            "Converted: NASA_2000.pdf\n",
            "Converted: NASA_2007.pdf\n",
            "Converted: NASA_2009.pdf\n",
            "Converted: NASA_2010.pdf\n",
            "Converted: NASA_2011.pdf\n",
            "Converted: NASA_2012.pdf\n",
            "Converted: NASA_2013.pdf\n",
            "Converted: NASA_2016.pdf\n",
            "Conversion completed for NASA\n",
            "Converted: NIH_1948.pdf\n",
            "Converted: NIH_1949.pdf\n",
            "Converted: NIH_1950.pdf\n",
            "Converted: NIH_1951.pdf\n",
            "Converted: NIH_1952.pdf\n",
            "Converted: NIH_1954.pdf\n",
            "Converted: NIH_1955.pdf\n",
            "Converted: NIH_1956.pdf\n",
            "Converted: NIH_1957.pdf\n",
            "Converted: NIH_1958.pdf\n",
            "Converted: NIH_1959.pdf\n",
            "Converted: NIH_1960.pdf\n",
            "Converted: NIH_1961.pdf\n",
            "Converted: NIH_1962.pdf\n",
            "Converted: NIH_1963.pdf\n",
            "Converted: NIH_1965.pdf\n",
            "Converted: NIH_1966.pdf\n",
            "Converted: NIH_1967.pdf\n",
            "Converted: NIH_1970.pdf\n",
            "Converted: NIH_1972.pdf\n",
            "Converted: NIH_1973.pdf\n",
            "Converted: NIH_1974.pdf\n",
            "Converted: NIH_1975.pdf\n",
            "Converted: NIH_1976.pdf\n",
            "Converted: NIH_1978.pdf\n",
            "Converted: NIH_1979.pdf\n",
            "Converted: NIH_1981.pdf\n",
            "Converted: NIH_1982.pdf\n",
            "Converted: NIH_1984.pdf\n",
            "Converted: NIH_1985.pdf\n",
            "Converted: NIH_1987.pdf\n",
            "Converted: NIH_1991.pdf\n",
            "Converted: NIH_1992.pdf\n",
            "Converted: NIH_1997.pdf\n",
            "Converted: NIH_1998.pdf\n",
            "Converted: NIH_1999.pdf\n",
            "Converted: NIH_2000.pdf\n",
            "Converted: NIH_2001.pdf\n",
            "Converted: NIH_2002.pdf\n",
            "Converted: NIH_2003.pdf\n",
            "Converted: NIH_2009.pdf\n",
            "Converted: NIH_2010.pdf\n",
            "Converted: NIH_2011.pdf\n",
            "Converted: NIH_2013.pdf\n",
            "Converted: NIH_2016.pdf\n",
            "Converted: NIH_2018.pdf\n",
            "Conversion completed for NIH\n",
            "Converted: NSF_1952.pdf\n",
            "Converted: NSF_1953.pdf\n",
            "Converted: NSF_1954.pdf\n",
            "Converted: NSF_1955.pdf\n",
            "Converted: NSF_1956.pdf\n",
            "Converted: NSF_1957.pdf\n",
            "Converted: NSF_1958.pdf\n",
            "Converted: NSF_1959.pdf\n",
            "Converted: NSF_1960.pdf\n",
            "Converted: NSF_1961.pdf\n",
            "Converted: NSF_1962.pdf\n",
            "Converted: NSF_1963.pdf\n",
            "Converted: NSF_1964.pdf\n",
            "Converted: NSF_1965.pdf\n",
            "Converted: NSF_1966.pdf\n",
            "Converted: NSF_1969.pdf\n",
            "Converted: NSF_1971.pdf\n",
            "Converted: NSF_1972.pdf\n",
            "Converted: NSF_1974.pdf\n",
            "Converted: NSF_1975.pdf\n",
            "Converted: NSF_1976.pdf\n",
            "Converted: NSF_1977.pdf\n",
            "Converted: NSF_1981.pdf\n",
            "Converted: NSF_1982.pdf\n",
            "Converted: NSF_1984.pdf\n",
            "Converted: NSF_1985.pdf\n",
            "Converted: NSF_1987.pdf\n",
            "Converted: NSF_1989.pdf\n",
            "Converted: NSF_1994.pdf\n",
            "Converted: NSF_1999.pdf\n",
            "Converted: NSF_2001.pdf\n",
            "Converted: NSF_2002.pdf\n",
            "Converted: NSF_2003.pdf\n",
            "Converted: NSF_2007.pdf\n",
            "Converted: NSF_2009.pdf\n",
            "Converted: NSF_2010.pdf\n",
            "Converted: NSF_2011.pdf\n",
            "Converted: NSF_2013.pdf\n",
            "Conversion completed for NSF\n",
            "Converted: Energy_1949.pdf\n",
            "Converted: Energy_1950.pdf\n",
            "Converted: Energy_1951.pdf\n",
            "Converted: Energy_1952.pdf\n",
            "Converted: Energy_1953.pdf\n",
            "Converted: Energy_1954.pdf\n",
            "Converted: Energy_1955.pdf\n",
            "Converted: Energy_1956.pdf\n",
            "Converted: Energy_1957.pdf\n",
            "Converted: Energy_1958.pdf\n",
            "Converted: Energy_1959.pdf\n",
            "Converted: Energy_1960.pdf\n",
            "Converted: Energy_1962.pdf\n",
            "Converted: Energy_1963.pdf\n",
            "Converted: Energy_1964.pdf\n",
            "Converted: Energy_1965.pdf\n",
            "Converted: Energy_1966.pdf\n",
            "Converted: Energy_1968.pdf\n",
            "Converted: Energy_1969.pdf\n",
            "Converted: Energy_1970.pdf\n",
            "Converted: Energy_1971.pdf\n",
            "Converted: Energy_1972.pdf\n",
            "Converted: Energy_1973.pdf\n",
            "Converted: Energy_1974.pdf\n",
            "Converted: Energy_1975.pdf\n",
            "Converted: Energy_1976.pdf\n",
            "Converted: Energy_1977.pdf\n",
            "Converted: Energy_1978.pdf\n",
            "Converted: Energy_1979.pdf\n",
            "Converted: Energy_1980.pdf\n",
            "Converted: Energy_1981.pdf\n",
            "Converted: Energy_1982.pdf\n",
            "Converted: Energy_1984.pdf\n",
            "Converted: Energy_1985.pdf\n",
            "Converted: Energy_1986.pdf\n",
            "Converted: Energy_1987.pdf\n",
            "Converted: Energy_1988.pdf\n",
            "Converted: Energy_1989.pdf\n",
            "Converted: Energy_1990.pdf\n",
            "Converted: Energy_1991.pdf\n",
            "Converted: Energy_1992.pdf\n",
            "Converted: Energy_1994.pdf\n",
            "Converted: Energy_1995.pdf\n",
            "Converted: Energy_1996.pdf\n",
            "Converted: Energy_1997.pdf\n",
            "Converted: Energy_1998.pdf\n",
            "Converted: Energy_1999.pdf\n",
            "Converted: Energy_2001.pdf\n",
            "Converted: Energy_2004.pdf\n",
            "Converted: Energy_2006.pdf\n",
            "Converted: Energy_2007.pdf\n",
            "Converted: Energy_2008.pdf\n",
            "Converted: Energy_2009.pdf\n",
            "Converted: Energy_2010.pdf\n",
            "Converted: Energy_2011.pdf\n",
            "Converted: Energy_2013.pdf\n",
            "Converted: Energy_2014.pdf\n",
            "Converted: Energy_2015.pdf\n",
            "Converted: Energy_2016.pdf\n",
            "Converted: Energy_2018.pdf\n",
            "Converted: Energy_2019.pdf\n",
            "Conversion completed for Energy\n",
            "Converted: Defense_1949.pdf\n",
            "Converted: Defense_1950.pdf\n",
            "Converted: Defense_1951.pdf\n",
            "Converted: Defense_1952.pdf\n",
            "Converted: Defense_1953.pdf\n",
            "Converted: Defense_1954.pdf\n",
            "Converted: Defense_1955.pdf\n",
            "Converted: Defense_1956.pdf\n",
            "Converted: Defense_1957.pdf\n",
            "Converted: Defense_1958.pdf\n",
            "Converted: Defense_1959.pdf\n",
            "Converted: Defense_1960.pdf\n",
            "Converted: Defense_1962.pdf\n",
            "Converted: Defense_1963.pdf\n",
            "Converted: Defense_1965.pdf\n",
            "Converted: Defense_1967.pdf\n",
            "Converted: Defense_1970.pdf\n",
            "Converted: Defense_1971.pdf\n",
            "Converted: Defense_1974.pdf\n",
            "Converted: Defense_1975.pdf\n",
            "Converted: Defense_1981.pdf\n",
            "Converted: Defense_1982.pdf\n",
            "Converted: Defense_1983.pdf\n",
            "Converted: Defense_1984.pdf\n",
            "Converted: Defense_1985.pdf\n",
            "Converted: Defense_1986.pdf\n",
            "Converted: Defense_1990.pdf\n",
            "Converted: Defense_1991.pdf\n",
            "Converted: Defense_1994.pdf\n",
            "Converted: Defense_1995.pdf\n",
            "Converted: Defense_2001.pdf\n",
            "Converted: Defense_2002.pdf\n",
            "Converted: Defense_2003.pdf\n",
            "Converted: Defense_2004.pdf\n",
            "Converted: Defense_2011.pdf\n",
            "Converted: Defense_2012.pdf\n",
            "Converted: Defense_2013.pdf\n",
            "Converted: Defense_2016.pdf\n",
            "Converted: Defense_2018.pdf\n",
            "Conversion completed for Defence\n"
          ]
        }
      ]
    }
  ]
}